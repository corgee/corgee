seed: 42
log_local_only: True

model:  # sets encoders for all data types and contrastive loss parameters to be used
  encoders:  # having stray encoders here doesn't hurt
    text1:
      type: "sbert"
      desc: "intfloat/multilingual-e5-base"
    text2: "text1"
  loss:
    losstrain:
      type: "nce"
      all_gather: True
      logit_scale: 4.0
      train_logit_scale: False
  find_unused_parameters: True
  bf16: true
exec:
  output_dir: "/data/sheshansh/_temp/test_output"
  train:
    save_interval: 100000
    forward_backward:
      emb_keys: "text1,text2"
      emb_dim: 768
      bf16: True
      ema:
        beta: 0.995  # override 0.0001, 0.99, 0.995, 0.999, ema should help?
        update_after_step: 1000
        update_every: 10
      scheduler:
        type: "warmuplinear"
        warmup_steps: 400
      optimizer:
        type: "adamw"
        lr: 0.00002  # override uniform [2e-5, 6e-5]...
        eps: 1.0e-06
        weight_decay: 0.01
      grad_clip_norm: 2.0
      max_forward_batch_size: 1024
    data:
      format: "binl_pairs_mt"
      data:
        files:
          data1:
            num_steps: 100
            maxlen1: 16
            maxlen2: 32
            file_pattern: "/data/sheshansh/_temp/test/*.tokbin"
        dtype: uint32
        padding_index: 1
        target: "losstrain:text1->text2"
      dl:
        batch_size: 32768
