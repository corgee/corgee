seed: 42
log_local_only: True

model:
  encoders:
    text1:
      type: "sbert"
      desc: "intfloat/multilingual-e5-base"
      load:
        path: "outputs/pretraining/models/model.pt"
        transformations:
            - replace: "module.=>"
            - filter: "encoders.text1."
            - replace_start: "encoders.text1.=>"
    text2: "text1"
  loss:
    losstrain:
      type: "nce"
      all_gather: True
      logit_scale: 4.0
      train_logit_scale: False
  find_unused_parameters: True
  bf16: true
exec:
  output_dir: "outputs/finetuning/"
  train:
    save_interval: 100000
    forward_backward:
      bf16: True
      ema:
        beta: 0.995
        update_after_step: 10
        update_every: 10
      scheduler:
        type: "warmuplinear"
        warmup_steps: 10
      optimizer:
        type: "adamw"
        lr: 0.0002
        eps: 1.0e-06
        weight_decay: 0.01
      grad_clip_norm: 2.0
    data:
      format: "binl_pairs_wnegs_mt"
      data:
        files:
          data1:
            num_steps: 200
            maxlen1: 16
            maxlen2: 64
            file_pattern: "resources/finetuning_data_tokenized/*.tokbin"
        num_negatives: 4
        dtype: uint32
        padding_index: 1
        target: "losstrain:text1->text2"
      dl:
        batch_size: 32
